\documentclass[UTF8]{ctexrep}
\ctexset{
    section/name = {第,节},
    section/number = {\arabic{section}},
    subsection/number = {\arabic{section}.\arabic{subsection}}
}

\title{郑和后台计划}
\author{zcy}

\bibliographystyle{acm}

\begin{document}
\maketitle
\newpage

    
%--------------------------%

\part{数据获取}

%--------------------------%

\part{数据存储}

%--------------------------%

\part{数据搜索}

\section{两大需求}

Aminer 提供快速，精确的搜索服务。以学者搜索为例，系统会按照学者的论文数、被引用数、h-index等指标排序并且能准确地区分\textbf{同名}的不同学者。

而在论文搜索中，Aminer 不仅仅能根据文章名进行搜索，还可以搜索与某一特定主题对应的论文。例如：直接搜索\textit{Data Mining}就会出现数据挖掘领域的杰出论文。\\

因此，郑和平台目前最主要的两个需求如下：

\begin{itemize}
    \item 根据关键词检索相关学者
    \item 根据关键词检索相关论文
\end{itemize}

考虑到用户体验和搜索结果的呈现质量，下述章节将基于这两大需求进行更为细致的分析，同时提供相应的解决方案。

\section{关键词预处理}

在查询数据库之前，至少应对用户输入进行如下处理：

\begin{itemize}
    \item 对用户输入内容的容错（例如拼写错误或输入错误）
    \item 对同义词、近义词的处理
    \item 对词型变化及格位变化的处理（词干提取）
    \item 分词
\end{itemize}

上述大部分都或多或少地与NLP相关。在项目开始的初期（目前），我们考虑使用成熟的开源实现。

\subsection{用户输入容错}

从后端方向考虑，郑和平台所采用的图数据库内建对Fuzzy Search的支持，但需要建立相应的特殊索引。

\subsection{同义词与近义词}

基于Word2Vec的实现方式，进行同义词识别。\\

同时列出一些较为流行的开源实现以供参考：

\begin{itemize}
    \item Synonyms\footnote{https://github.com/huyingxi/Synonyms}，预训练的中文相关词汇实现
    \item wordvectors\footnote{https://github.com/Kyubyong/wordvectors}，支持多语言的Word2Vec实现
\end{itemize}

\subsection{分词}

英文语境下的分词依照空格分割之后去除介词和代词。

若来自用户的中文输入为句子（考虑到并非所有用户会用分隔符将关键词分开），首先应进行分词。

开源社区已有极为优秀和高效的中文分词实现jieba\footnote{https://github.com/fxsjy/jieba}，具有如下优势：

\begin{itemize}
    \item 基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG)
    \item 采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合
    \item 对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法
\end{itemize}

\subsection{词干提取}

相比分词，在中文语境下对用户输入进行词干提取的实用意义不是很大（直觉得出，尚无数据佐证）。

而在英文语境下，对关键词进行词干提取是必要的。例如，以关键词“optimization”和“optimized”分别进行搜索被期待返回接近或者相同的查询结果。事实上，Google和Bing等搜索引擎确实对用户输入进行了词干提取。

采用Porter算法\cite{porter2006algorithm}进行词干提取效果较好，论文作者同时提供了免费的改进项目Snowball\footnote{https://snowballstem.org/}。

\section{权重与优先级}

对于每次搜索，用户输入经过预处理后应具有类似如下的结构：

$$i=\{p,\ p[\ ],\ s[\ ],\ r[\ ]\}$$

$p$为用户的原始输入字符串，$p[\ ]$为分词结果，$s[\ ]$为词干提取结果，$r[\ ]$为相关词汇。

对$i$中每个元素赋予不同的权重$w_n$，依据$\sum{w_n}$决定搜索结果的相关程度。具体权重的分配方法仍需进一步讨论。

\section{搜索词频统计}

在数据库层面进行持久化，对每次搜索过程的分词结果$p[\ ]$和词干提取结果$s[\ ]$进行索引。

定期分析搜索日志，进行进一步分析（？）

\section{热点数据缓存}

为减轻服务器压力，应基于搜索词频统计结果，将热点数据缓存。

详见数据库部分。

%--------------------------%

\bibliography{papers}

%--------------------------%

\end{document}
